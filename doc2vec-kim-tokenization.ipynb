{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec with how Yoon Kim did it\n",
    "\n",
    "Steps:\n",
    "* Tokenize punctuations as if they are their own words\n",
    "* Determine the longest review's word count, then pad other reviews so that they are all as long as the longest review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 1060 6GB (CNMeM is disabled, cuDNN 5105)\n",
      "/home/anonoz/anaconda2/envs/tensorflow/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import gensim\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Easily changable settings\n",
    "# We will only train from training/unlabeled set\n",
    "text_corpus_files = ['aclImdb/train/pos/*.txt', 'aclImdb/train/neg/*.txt', 'aclImdb/train/unsup/*.txt']\n",
    "word_vector_dims = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    #1 Remove HTML (inspired by Kaggle)\n",
    "    text = BeautifulSoup(text, \"html.parser\").getText()\n",
    "\n",
    "    #2 Tokenize (stolen from Yoon Kim's CNN)\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", text)     \n",
    "    text = re.sub(r\"\\'s\", \" \\'s\", text) \n",
    "    text = re.sub(r\"\\'ve\", \" \\'ve\", text) \n",
    "    text = re.sub(r\"n\\'t\", \" n\\'t\", text) \n",
    "    text = re.sub(r\"\\'re\", \" \\'re\", text) \n",
    "    text = re.sub(r\"\\'d\", \" \\'d\", text) \n",
    "    text = re.sub(r\"\\'ll\", \" \\'ll\", text) \n",
    "    text = re.sub(r\",\", \" , \", text) \n",
    "    text = re.sub(r\"!\", \" ! \", text) \n",
    "    text = re.sub(r\"\\(\", \" \\( \", text) \n",
    "    text = re.sub(r\"\\)\", \" \\) \", text) \n",
    "    text = re.sub(r\"\\?\", \" \\? \", text) \n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    #3 Lower cap\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_text_list(text_list, pad_token=\"<PAD/>\", pad_width=0):\n",
    "    return text_list + ([pad_token] * (pad_width - len(text_list)))\n",
    "\n",
    "def text_to_padded_list(text, pad_token=\"<PAD/>\", pad_width=0):\n",
    "    text_list = preprocess_text(text).split()\n",
    "    return pad_text_list(text_list, pad_token, pad_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text file 75000\n",
      "Longest text list: 2773\n"
     ]
    }
   ],
   "source": [
    "processed_texts = []\n",
    "file_names = []\n",
    "file_count = 0\n",
    "for folder_files in text_corpus_files:\n",
    "    for text_file in glob.glob(folder_files):\n",
    "        with(open(text_file, 'r')) as f:\n",
    "            processed_texts.append(text_to_padded_list(f.read()))\n",
    "            file_names.append(text_file)\n",
    "            file_count += 1\n",
    "            if file_count % 100 == 0:\n",
    "                sys.stdout.write('\\rLoading text file {0:d}'.format(file_count))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "max_processed_text_len = len(max(processed_texts, key=len))\n",
    "print('\\nLongest text list: {0:d}'.format(max_processed_text_len))\n",
    "# for i, text_list in enumerate(processed_texts):\n",
    "#     processed_texts[i] = pad_text_list(text_list, pad_width=max_processed_text_len)\n",
    "#     if (i + 1) % 1000 == 0:\n",
    "#         sys.stdout.write('\\rPadding text list {0:d}'.format(i+1))\n",
    "#         sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabeledReview(object):\n",
    "    def __init__(self, docs_list, labels_list):\n",
    "        self.docs_list = docs_list\n",
    "        self.labels_list = labels_list\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.docs_list):\n",
    "            yield TaggedDocument(words=doc, tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning epoch 1\n",
      "Beginning epoch 2\n",
      "Beginning epoch 3\n",
      "Beginning epoch 4\n",
      "Beginning epoch 5\n",
      "Beginning epoch 6\n",
      "Beginning epoch 7\n",
      "Beginning epoch 8\n",
      "Beginning epoch 9\n",
      "Beginning epoch 10\n"
     ]
    }
   ],
   "source": [
    "it = LabeledReview(processed_texts, file_names)\n",
    "\n",
    "model = Doc2Vec(size=100, window=8, min_count=1, workers=4, alpha=0.025, min_alpha=0.025)\n",
    "model.build_vocab(it)\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"Beginning epoch {0:d}\".format(epoch+1))\n",
    "    model.train(it)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha\n",
    "    model.train(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_word2vec_format('word2vec/d2v-not-padded-300d.bin', binary=True)\n",
    "model.save('word2vec/d2v-not-padded-300d.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test loading from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_model = gensim.models.Doc2Vec.load_word2vec_format('word2vec/d2v-padded.bin', binary=True)\n",
    "test_model = Doc2Vec.load('word2vec/d2v-not-padded-300d.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'maniac', 0.7654415965080261),\n",
       " (u'geek', 0.739655077457428),\n",
       " (u'demon', 0.7359193563461304),\n",
       " (u'prostitute', 0.7281798720359802),\n",
       " (u'doctor', 0.7246631383895874),\n",
       " (u'dog', 0.7230076193809509),\n",
       " (u'policeman', 0.7188103199005127),\n",
       " (u'lawyer', 0.7156832218170166),\n",
       " (u'psychopath', 0.7153109312057495),\n",
       " (u'bird', 0.7108577489852905)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.most_similar('robot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer_vector(text):\n",
    "    test_model.infer_vector(text_to_padded_list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infer_vector('Apple decides to kill ornage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.74797489e-03,  -1.32032363e-02,   1.30325938e-02,\n",
       "         1.42958378e-02,  -1.42836208e-02,  -5.90778328e-02,\n",
       "         2.33219527e-02,  -2.11221278e-02,   5.03734611e-02,\n",
       "        -3.08535080e-02,  -1.44617874e-02,  -1.04359677e-02,\n",
       "         4.01574671e-02,   7.72434473e-03,  -1.30693289e-03,\n",
       "         1.57768670e-02,  -2.20668390e-02,  -4.21150662e-02,\n",
       "         1.96832586e-02,   4.25131656e-02,  -2.02304125e-02,\n",
       "         4.90427576e-02,   1.87942851e-02,  -5.24087213e-02,\n",
       "         3.67905851e-03,   1.68974716e-02,   1.02010611e-02,\n",
       "        -6.92933053e-03,  -8.07955116e-03,  -3.62554900e-02,\n",
       "        -2.07417347e-02,  -4.66159768e-02,  -1.19143194e-02,\n",
       "        -1.00062266e-02,  -7.33764516e-03,  -1.53856128e-02,\n",
       "         7.23951906e-02,   3.38784128e-04,   1.08476970e-02,\n",
       "         1.61488727e-02,  -2.71307435e-02,   5.93591258e-02,\n",
       "         2.09669843e-02,  -3.16876685e-04,   2.54739262e-02,\n",
       "        -1.11382911e-02,   5.29330447e-02,  -1.56346373e-02,\n",
       "        -7.49982754e-03,  -2.04776283e-02,  -7.07785636e-02,\n",
       "         3.02374698e-02,   2.59031784e-02,   2.65554208e-02,\n",
       "         6.71031978e-03,  -1.50858704e-02,   2.92365123e-02,\n",
       "        -9.62415070e-05,   1.58715062e-03,   5.62401898e-02,\n",
       "        -2.23711934e-02,   6.28273115e-02,  -3.24410684e-02,\n",
       "         4.82694954e-02,   1.92828104e-02,   4.75162864e-02,\n",
       "         1.96302347e-02,  -3.34008597e-02,   2.03270768e-03,\n",
       "         2.25487147e-02,   1.35731092e-02,   8.21422227e-03,\n",
       "         3.38265188e-02,  -2.39650942e-02,  -3.45473923e-02,\n",
       "         6.23109490e-02,   4.93579321e-02,  -4.36835103e-02,\n",
       "         8.54312163e-03,  -1.78194344e-02,  -5.37556633e-02,\n",
       "        -2.63106171e-02,   4.72629629e-03,   3.79304513e-02,\n",
       "         1.45030264e-02,  -1.34499595e-02,   4.74444479e-02,\n",
       "         1.20453853e-02,  -1.19143724e-02,   1.82929747e-02,\n",
       "        -1.47775467e-02,  -6.14486858e-02,   5.31507544e-02,\n",
       "         3.51709053e-02,  -3.83559316e-02,   1.42590441e-02,\n",
       "         1.77561752e-02,  -1.49466854e-04,   6.64008269e-03,\n",
       "         2.66542602e-02], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.infer_vector('what the fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
