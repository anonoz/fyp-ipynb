{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec (DBOW) on IMDB dataset\n",
    "\n",
    "Steps:\n",
    "* Tokenize punctuations as if they are their own words\n",
    "* Determine the longest review's word count, then pad other reviews so that they are all as long as the longest review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 1060 6GB (CNMeM is disabled, cuDNN 5105)\n",
      "/home/anonoz/anaconda2/envs/tensorflow/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import gensim\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Easily changable settings\n",
    "# We will only train from training/unlabeled set\n",
    "text_corpus_files = ['aclImdb/train/pos/*.txt', 'aclImdb/train/neg/*.txt', 'aclImdb/train/unsup/*.txt']\n",
    "word_vector_dims = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    #1 Remove HTML (inspired by Kaggle)\n",
    "    text = BeautifulSoup(text, \"html.parser\").getText()\n",
    "\n",
    "    #2 Tokenize (stolen from Yoon Kim's CNN)\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", text)     \n",
    "    text = re.sub(r\"\\'s\", \" \\'s\", text) \n",
    "    text = re.sub(r\"\\'ve\", \" \\'ve\", text) \n",
    "    text = re.sub(r\"n\\'t\", \" n\\'t\", text) \n",
    "    text = re.sub(r\"\\'re\", \" \\'re\", text) \n",
    "    text = re.sub(r\"\\'d\", \" \\'d\", text) \n",
    "    text = re.sub(r\"\\'ll\", \" \\'ll\", text) \n",
    "    text = re.sub(r\",\", \" , \", text) \n",
    "    text = re.sub(r\"!\", \" ! \", text) \n",
    "    text = re.sub(r\"\\(\", \" \\( \", text) \n",
    "    text = re.sub(r\"\\)\", \" \\) \", text) \n",
    "    text = re.sub(r\"\\?\", \" \\? \", text) \n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    #3 Lower cap\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_text_list(text_list, pad_token=\"<PAD/>\", pad_width=0):\n",
    "    return text_list + ([pad_token] * (pad_width - len(text_list)))\n",
    "\n",
    "def text_to_padded_list(text, pad_token=\"<PAD/>\", pad_width=0):\n",
    "    text_list = preprocess_text(text).split()\n",
    "    return pad_text_list(text_list, pad_token, pad_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text file 75000\n",
      "Longest text list: 2773\n"
     ]
    }
   ],
   "source": [
    "processed_texts = []\n",
    "file_names = []\n",
    "file_count = 0\n",
    "for folder_files in text_corpus_files:\n",
    "    for text_file in glob.glob(folder_files):\n",
    "        with(open(text_file, 'r')) as f:\n",
    "            processed_texts.append(text_to_padded_list(f.read()))\n",
    "            file_names.append(text_file)\n",
    "            file_count += 1\n",
    "            if file_count % 100 == 0:\n",
    "                sys.stdout.write('\\rLoading text file {0:d}'.format(file_count))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "max_processed_text_len = len(max(processed_texts, key=len))\n",
    "print('\\nLongest text list: {0:d}'.format(max_processed_text_len))\n",
    "# for i, text_list in enumerate(processed_texts):\n",
    "#     processed_texts[i] = pad_text_list(text_list, pad_width=max_processed_text_len)\n",
    "#     if (i + 1) % 1000 == 0:\n",
    "#         sys.stdout.write('\\rPadding text list {0:d}'.format(i+1))\n",
    "#         sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabeledReview(object):\n",
    "    def __init__(self, docs_list, labels_list):\n",
    "        self.docs_list = docs_list\n",
    "        self.labels_list = labels_list\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.docs_list):\n",
    "            yield TaggedDocument(words=doc, tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning epoch 1\n",
      "Beginning epoch 2\n",
      "Beginning epoch 3\n",
      "Beginning epoch 4\n",
      "Beginning epoch 5\n",
      "Beginning epoch 6\n",
      "Beginning epoch 7\n",
      "Beginning epoch 8\n",
      "Beginning epoch 9\n",
      "Beginning epoch 10\n",
      "--- 1799.02837205 seconds ---\n"
     ]
    }
   ],
   "source": [
    "it = LabeledReview(processed_texts, file_names)\n",
    "\n",
    "model = Doc2Vec(size=300, window=8, min_count=1, workers=4, alpha=0.025, min_alpha=0.025, dm=0)\n",
    "model.build_vocab(it)\n",
    "\n",
    "# TIMER\n",
    "import time\n",
    "start_time = time.time()\n",
    "# END TIMER\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"Beginning epoch {0:d}\".format(epoch+1))\n",
    "    model.train(it)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha\n",
    "    model.train(it)\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('word2vec/d2v-dbow-not-padded-300d.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test loading from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_model = gensim.models.Doc2Vec.load_word2vec_format('word2vec/d2v-padded.bin', binary=True)\n",
    "test_model = Doc2Vec.load('word2vec/d2v-not-padded-300d.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'maniac', 0.7654415965080261),\n",
       " (u'geek', 0.739655077457428),\n",
       " (u'demon', 0.7359193563461304),\n",
       " (u'prostitute', 0.7281798720359802),\n",
       " (u'doctor', 0.7246631383895874),\n",
       " (u'dog', 0.7230076193809509),\n",
       " (u'policeman', 0.7188103199005127),\n",
       " (u'lawyer', 0.7156832218170166),\n",
       " (u'psychopath', 0.7153109312057495),\n",
       " (u'bird', 0.7108577489852905)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.most_similar('robot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer_vector(text):\n",
    "    test_model.infer_vector(text_to_padded_list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infer_vector('Apple decides to kill ornage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-adfafc6f269c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Can also accept string but who knows?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_model.infer_vector('Can also accept string but who knows?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
